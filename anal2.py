from sklearn.feature_extraction.text import CountVectorizer
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
import numpy as np
import re
from nltk.corpus import stopwords

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä CountVectorizer —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —è–∑—ã–∫–∞
max_feature = 400
vectorizer = CountVectorizer(token_pattern=r'\b[–∞-—è–ê-–Ø—ë–Å]+\b', max_features=max_feature)

# –ß–∏—Ç–∞–µ–º –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –∏–∑ —Ñ–∞–π–ª–æ–≤
with open("pos.txt", "r", encoding='utf-8') as f:
    positive_texts = f.readlines()
with open("neg.txt", "r", encoding='utf-8') as f:
    negative_texts = f.readlines()

stop_words_set = set(stopwords.words('russian'))
stop_words_set.add('—ç—Ç–æ')
stop_words_set.add('–æ–Ω–æ') 
stop_words_set.add('—ç—Ç–∞')
#print(stop_words_set)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–¥–Ω–æ–∫–æ—Ä–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤
def is_related(word1, word2):
    return word1[:-2] == word2[:-2]

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö —Å–ª–æ–≤
def filter_top_words(top_words):
    filtered_words = []
    for weight, word in top_words:
        if (word not in stop_words_set and not re.match(r'^\d', word) and
                not any(is_related(word, filtered_word) for _, filtered_word in filtered_words)):
            filtered_words.append((weight, word))
    return filtered_words

# –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –≤—ã–±–æ—Ä–∫—É
X_train = positive_texts + negative_texts
y_train = ['positive'] * len(positive_texts) + ['negative'] * len(negative_texts)

X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, stratify=y_train)

# –§–æ—Ä–º–∏—Ä—É–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏
X_train_vectorized = vectorizer.fit_transform(X_train)

# –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å
clf = MLPClassifier(hidden_layer_sizes=(50,), max_iter=100, verbose = True, learning_rate_init=0.0001, batch_size=25, alpha=0.1)
clf.fit(X_train_vectorized, y_train)

# –ß–∏—Ç–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –∏–∑ —Ñ–∞–π–ª–∞
with open("test_texts.txt", "r", encoding='utf-8') as f:
    test_texts = f.readlines()

# –§–æ—Ä–º–∏—Ä—É–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
X_test_vectorized = vectorizer.transform(test_texts)

# –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤
y_pred = clf.predict(X_test_vectorized)
y_pred_proba = clf.predict_proba(X_test_vectorized)
total_accuracy = np.mean([1 if true == pred else 0 for true, pred in zip(y_test, y_pred)])

a = 0
b = 0
# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
for text, pred, prob in zip(test_texts, y_pred, y_pred_proba):
    if pred == 'positive':
        #print(f"–¢–µ–∫—Å—Ç: {text.strip()}")
        #print(f"–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:{'%.2f' % (prob[1]*100)}% –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π")
        a = float('%.2f' % (prob[1]*100)) + a
    else:
        #print(f"–¢–µ–∫—Å—Ç: {text.strip()}")
        #print(f"–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:{'%.2f' % (prob[0]*100)}% –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π")
        b = float('%.2f' % (prob[0]*100)) + b
print(f"–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å = {(a+b)/20}%")
#print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤: {int(total_accuracy * len(y_test))} –∏–∑ {len(y_test)}")


# –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –∏—Ö –≤–µ—Å–∞
feature_names = vectorizer.get_feature_names()

# –§–∏–ª—å—Ç—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Å–ª–æ–≤–∞, –∏—Å–∫–ª—é—á–∞—è —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –æ–¥–Ω–æ–∫–æ—Ä–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
top_words = [(weight, word) for weight, word in zip(clf.coefs_[-1], feature_names)]
top_words = filter_top_words(top_words)
top_words = sorted(top_words, reverse=True)

#print("\n–ù–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Å–ª–æ–≤–∞:")
#for weight, word in top_words:
#    print(f"{word}: ùúî={weight}")

# –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–π –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ
word_counts = {}
for i, word in enumerate(feature_names):
    if word not in stop_words_set and not re.match(r'^\d', word):
        filtered = False
        for _, filtered_word in top_words:
            if is_related(word, filtered_word):
                filtered = True
                break
        if not filtered:
            word_counts[word] = word_counts.get(word, 0) + X_train_vectorized.getcol(i).sum()
                                                                                        
# –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ–ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞ –∏ –∏—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ä–µ–¥–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö —Å–ª–æ–≤
#top_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:30]
#print("\n–ù–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞ —Å—Ä–µ–¥–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö:")
#for word, count in top_counts:
#    print(f"{word}: {count}")